{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlJ4yKeCvrY2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "import os\n",
        "import numpy as np\n",
        "import xarray\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import pandas as pd\n",
        "from scipy.special import ndtri,ndtr\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from keras.src.layers import concatenate\n",
        "from keras.regularizers import L1,L2,L1L2\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "INPUT_SHAPE = (None, 72, 9, 9, 7)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the ERA5 dataset\n",
        "Dataset=xarray.open_dataset(\"everything.nc\")\n",
        "date=Dataset.time.values[5:-19]-np.timedelta64(5,'h')\n",
        "\n",
        "# Convert the xarray dataset to a numpy array\n",
        "data_array = Dataset.to_array()\n",
        "data_array = data_array.to_numpy()\n",
        "data_array = np.moveaxis(data_array, 0, -1)[5:-19,:,:,:]\n",
        "\n",
        "print(data_array.shape) #timestep,lat,lon,variable\n",
        "Y=np.copy(data_array[63::24,3,3,3]) #create the output variable\n",
        "month = date.astype('datetime64[M]').astype(int) % 4 #Transform the date data into the local \"seasons\"\n",
        "month=month[87::24]\n",
        "\n",
        "#create the input data\n",
        "X=np.copy(data_array[14:-10,:,:,:]).reshape(-1,24,9,9,7)\n",
        "X=np.reshape(X,(-1,9,9,7))\n",
        "M=np.lib.stride_tricks.sliding_window_view(X,window_shape=72,axis=0)[::24, :,:,:] #create a sliding window view to save space\n",
        "M=np.reshape(M,(-1,9,9,7,72))\n",
        "M=np.rollaxis(M,-1,1)\n",
        "print(M.shape) # data entries, 72 timesteps, lat, lon, variables\n"
      ],
      "metadata": {
        "id": "ZV9KnXf2hQ3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pre processing\n",
        "\n",
        "#Standarize precipitation\n",
        "Y[Y< 0] = 0 #eliminate negative values of precipitation\n",
        "u=1-np.exp(-Y/np.mean(Y))\n",
        "u[u>0.999]=0.999\n",
        "u[u<0.05]=0.05\n",
        "u=ndtri(u)\n",
        "def inverse(u,y):\n",
        "  \"Returns from the standarized precipitation to milimeters of rain\"\n",
        "  return -np.log(1-ndtr(u))*np.mean(y)\n",
        "\n",
        "#Additional information (season and last day precipitation at the same hour)\n",
        "xb = np.concatenate((month.reshape(-1,1),u[:-1].reshape(-1,1)),axis=1)\n",
        "\n",
        "\n",
        "#separate into training and testing\n",
        "X_train=M[:13145,:,:,:]\n",
        "X_test=M[13145:,:,:,:]\n",
        "Y_train=u[1:13146]\n",
        "Y_test=u[13146:]\n",
        "xb_train=xb[:13145]\n",
        "xb_test=xb[13145:]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "alWBa96ZhYEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the base model\n",
        "tf.keras.utils.set_random_seed(\n",
        "    153\n",
        ")\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'base_model.keras',  # Path where the best model will be saved\n",
        "    monitor='val_mse',  # Monitor validation loss (you can change this to 'val_mae' if you prefer MAE)\n",
        "    save_best_only=True,  # Save only the best model (according to validation loss)\n",
        "    save_weights_only=False,  # Save the entire model, not just weights\n",
        "    verbose=0  # Print information when the model is saved\n",
        ")\n",
        "\n",
        "#from keras.src.layers import concatenate\n",
        "with strategy.scope():\n",
        "  input = tf.keras.Input(shape=INPUT_SHAPE[1:])\n",
        "  inputB = tf.keras.Input(shape=(None,4)[1:])\n",
        "\n",
        "  x = input\n",
        "  x = tf.keras.layers.Conv3D(filters=42, kernel_size=(3, 2, 2), padding='same',groups=7,use_bias=False)(x)\n",
        "  x = tf.keras.activations.relu(x)\n",
        "  x = tf.keras.layers.Conv3D(filters=16, kernel_size=(9, 3, 3), padding='valid',use_bias=False)(x)\n",
        "  x = tf.keras.activations.sigmoid(x)\n",
        "  x = tf.keras.layers.Conv3D(filters=20, kernel_size=(12, 4, 4), padding='valid',use_bias=False)(x)\n",
        "  x = tf.keras.activations.relu(x)\n",
        "  x1 = layers.Flatten()(x)\n",
        "  x1 = layers.Dropout(0.5)(x1)\n",
        "  x1 = layers.Dense(4,use_bias=False,activation='sigmoid')(x1)\n",
        "\n",
        "  x = tf.keras.layers.Conv3D(filters=24, kernel_size=(24, 4, 4), padding='valid',use_bias=False)(x)\n",
        "  x = tf.keras.activations.relu(x)\n",
        "\n",
        "  x = layers.Flatten()(x)\n",
        "\n",
        "  x = layers.Dense(40,use_bias=False,activation='relu')(x)\n",
        "  x = keras.Model(inputs=input, outputs=x)\n",
        "  x1= keras.Model(inputs=input, outputs=x1)\n",
        "  y=inputB\n",
        "  y = keras.Model(inputs=inputB, outputs=y)\n",
        "\n",
        "  z = concatenate([x.output, y.output,x1.output])\n",
        "  z = layers.Dense(32,use_bias=False,activation='relu')(z)\n",
        "  z = layers.Dense(1,use_bias=True,activation='linear')(z)\n",
        "  model = keras.Model(inputs=[input,inputB], outputs=z)\n",
        "\n",
        "  #model.build(frames)\n",
        "  model.compile(loss='mean_squared_error',\n",
        "              optimizer=keras.optimizers.Adam(),\n",
        "               metrics=['root_mean_squared_error'])\n",
        "\n",
        "history = model.fit(x=[M,x_b_train],\n",
        "                    epochs=80,\n",
        "                    y=y_train, validation_split=0.2,batch_size=256, callbacks=[checkpoint],shuffle=True)\n",
        "model.load_weights(\"base_model.keras\")\n"
      ],
      "metadata": {
        "id": "8eyog2WBjptZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sCAdQezFwGd9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Prepare the data for the transfer learning model\n",
        "\n",
        "df=pd.read_csv(\"df_database_toreadora_complete_corrected.csv\",usecols=[\"TIMESTAMP\",\"Rain_mm_Tot\"])\n",
        "df=df.fillna(0)\n",
        "df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])\n",
        "\n",
        "# Extract the date part of the datetime column\n",
        "df['date'] = df['TIMESTAMP'].dt.date\n",
        "\n",
        "# Group by the 'date' column and sum the rain values for each day\n",
        "daily_rain = df.groupby('date')['Rain_mm_Tot'].sum().reset_index()\n",
        "\n",
        "zerorain=0.5\n",
        "rain=daily_rain['Rain_mm_Tot'].to_numpy()\n",
        "\n",
        "p33 = daily_rain['Rain_mm_Tot'].quantile(0.33)\n",
        "p66 = daily_rain['Rain_mm_Tot'].quantile(0.66)\n",
        "p90 = daily_rain['Rain_mm_Tot'].quantile(0.90)\n",
        "categorized_rain=np.zeros(len(daily_rain))\n",
        "categorized_rain+=rain>p33\n",
        "categorized_rain+=rain>p66\n",
        "categorized_rain+=rain>p90\n",
        "categorized_rain=categorized_rain.astype(int)\n",
        "categorized_rain=tf.one_hot(categorized_rain,depth=4)\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced',classes=np.array([0,1,2,3]),y=np.argmax(categorized_rain,axis=1))\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm7TLHVU4XwI"
      },
      "outputs": [],
      "source": [
        "X_transfer=M[-3662:]\n",
        "xb_transfer=xb[-3662:]\n",
        "y_transfer=categorized_rain[:3662,:].numpy()\n",
        "\n",
        "#Separate into train and testing datasets\n",
        "X_train_transfer=X_transfer[365*6:]\n",
        "xb_train_transfer=xb_transfer[365*6:]\n",
        "y_train_transfer=y_transfer[365*6:]\n",
        "X_test_transfer=X_transfer[:365*6]\n",
        "xb_test_transfer=xb_transfer[:365*6]\n",
        "y_test_transfer=y_transfer[:365*6]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the transfer learning model\n",
        "with strategy.scope():\n",
        "# Freeze all layers except dense4 and dense5\n",
        "  for layer in model.layers[:]:\n",
        "      layer.trainable = False\n",
        "  #for layer in model.layers[-2:]:\n",
        "  #    layer.trainable = True\n",
        "  #model.layers[-4].trainable=True\n",
        "  #model.layers[-6].trainable=True\n",
        " #add a relu layer at the end\n",
        "  inputs = model.inputs  # Get the inputs of the original model\n",
        "  z1=model.layers[-8].output\n",
        "  z1=layers.Dense(8,activation='leaky_relu',kernel_regularizer=L1L2(l1=1e-5, l2=1e-6))(z1)\n",
        "  z2=model.layers[-9].output\n",
        "  z2=layers.Dense(8,activation='leaky_relu',kernel_regularizer=L1L2(l1=5e-3, l2=5e-4))(z2)\n",
        "  z3=model.layers[-5].output\n",
        "  z3=layers.Dense(8,activation='leaky_relu')(z3)\n",
        "  z=concatenate([z1,z2,z3])\n",
        "  z=layers.Dense(8,activation='tanh')(z)\n",
        "  z=layers.Dense(4,use_bias=True,activation='softmax')(z)\n",
        "\n",
        "  new_model = keras.Model(inputs=inputs, outputs=z)\n",
        "# Print the trainable status of each layer to verify\n",
        "for layer in new_model.layers:\n",
        "  print(layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "LPdoCh-AXBFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint(\n",
        "    'final_model.keras',  # Path where the best model will be saved\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,  # Save only the best model\n",
        "    save_weights_only=False,  # Save the entire model, not just weights\n",
        "    verbose=0  # Print information when the model is saved\n",
        ")\n",
        "\n",
        "with strategy.scope():\n",
        "    # Compile the model again after setting trainable layers\n",
        "    new_model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalFocalCrossentropy(\n",
        "    alpha=class_weights,\n",
        "    gamma=2.5,\n",
        "    from_logits=False,\n",
        "    label_smoothing=0.0,\n",
        "    axis=-1,\n",
        "    reduction='sum_over_batch_size',\n",
        "    name='categorical_focal_crossentropy'\n",
        ")\n",
        "    , metrics=['accuracy'])  # Choose appropriate optimizer, loss, and metrics\n",
        "\n",
        "    # Train the model with the checkpoint callback\n",
        "    history = new_model.fit(\n",
        "        x=[X_train_transfer, xb_train_transfer],\n",
        "        y=y_train_transfer,\n",
        "        epochs=60,  # Adjust the number of epochs as needed\n",
        "        batch_size=128, # Adjust the batch size\n",
        "        validation_split=0.1,  # Use a validation split\n",
        "        verbose=2,\n",
        "        callbacks=[checkpoint]  # Add the checkpoint callback here\n",
        "        #,class_weight=class_weight\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    loss, accuracy = new_model.evaluate([X_test_transfer, xb_test_transfer], y_test_transfer, verbose=0)\n",
        "    print(f\"Mean Absolute Error on test set: {accuracy}\")"
      ],
      "metadata": {
        "id": "VgxS255fXgnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iqnbb_XXbE6e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}